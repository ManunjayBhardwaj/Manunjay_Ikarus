{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995b6ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Train text-embedding based recommendation + clustering on the cleaned dataset.\n",
    "\n",
    "Inputs:\n",
    "  --data backend/data/cleaned_furniture.csv\n",
    "\n",
    "Artifacts (default --out backend/storage):\n",
    "  - embeddings.npy               (float32, shape [N, D])\n",
    "  - faiss_index.bin             (FAISS cosine index)\n",
    "  - metadata.json               (list of product dicts aligned with embeddings)\n",
    "  - cluster_labels.csv          (uniq_id, cluster_label)\n",
    "  - training_report.json        (summary: rows, dims, timing, silhouette, etc.)\n",
    "\n",
    "Usage:\n",
    "  python train_models.py --data backend/data/cleaned_furniture.csv --out backend/storage --clusters 12\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Vector index\n",
    "try:\n",
    "    import faiss  # faiss-cpu\n",
    "except Exception as e:\n",
    "    print(\"[train] ERROR: faiss not installed. Install faiss-cpu\", flush=True)\n",
    "    raise\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "\n",
    "def log(msg: str):\n",
    "    print(f\"[train] {msg}\", flush=True)\n",
    "\n",
    "def read_cleaned_csv(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Missing dataset: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    # Ensure expected normalized columns exist (from your cleaner script)\n",
    "    expected = [\n",
    "        \"uniq_id\", \"title_norm\", \"brand_norm\", \"description_norm\", \"price_num\",\n",
    "        \"categories_norm\", \"primary_image\", \"material_std\", \"color_std\"\n",
    "    ]\n",
    "    for c in expected:\n",
    "        if c not in df.columns:\n",
    "            log(f\"WARNING: column '{c}' not found; creating placeholder.\")\n",
    "            df[c] = np.nan\n",
    "    # Minimal cleanups\n",
    "    df[\"title_norm\"] = df[\"title_norm\"].fillna(\"Unknown Title\").astype(str)\n",
    "    df[\"brand_norm\"] = df[\"brand_norm\"].fillna(\"Unknown\").astype(str)\n",
    "    df[\"description_norm\"] = df[\"description_norm\"].fillna(\"No description available.\").astype(str)\n",
    "    df[\"material_std\"] = df[\"material_std\"].fillna(\"unknown\").astype(str)\n",
    "    df[\"color_std\"] = df[\"color_std\"].fillna(\"unknown\").astype(str)\n",
    "    # categories_norm might be a list serialized as string -> try to parse light\n",
    "    def parse_cats(x):\n",
    "        if pd.isna(x): return []\n",
    "        s = str(x).strip()\n",
    "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "            try:\n",
    "                import ast\n",
    "                v = ast.literal_eval(s)\n",
    "                if isinstance(v, list): return [str(t) for t in v]\n",
    "            except Exception:\n",
    "                pass\n",
    "        # fallback: split on commas\n",
    "        return [t.strip() for t in s.split(\",\") if t.strip()]\n",
    "    df[\"categories_list\"] = df[\"categories_norm\"].apply(parse_cats)\n",
    "    return df\n",
    "\n",
    "def build_corpus_row(r: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Compose a compact text for embedding that captures key product signals.\n",
    "    \"\"\"\n",
    "    cats = \" > \".join(r.get(\"categories_list\", [])[:3])\n",
    "    parts = [\n",
    "        r.get(\"title_norm\", \"\"),\n",
    "        f\"Brand: {r.get('brand_norm', '')}\",\n",
    "        f\"Material: {r.get('material_std', '')}\",\n",
    "        f\"Color: {r.get('color_std', '')}\",\n",
    "        f\"Categories: {cats}\" if cats else \"\",\n",
    "        r.get(\"description_norm\", \"\")\n",
    "    ]\n",
    "    return \" | \".join([p for p in parts if p])\n",
    "\n",
    "def embed_texts(model: SentenceTransformer, texts: List[str], batch_size: int = 128) -> np.ndarray:\n",
    "    embs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        chunk = texts[i:i+batch_size]\n",
    "        embs.append(model.encode(chunk, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=False))\n",
    "    X = np.vstack(embs).astype(\"float32\")\n",
    "    return X\n",
    "\n",
    "def l2_normalize(vectors: np.ndarray) -> np.ndarray:\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True) + 1e-12\n",
    "    return vectors / norms\n",
    "\n",
    "def build_faiss_cosine_index(vectors_unit: np.ndarray):\n",
    "    \"\"\"\n",
    "    Cosine similarity with FAISS: use IndexFlatIP on L2-normalized vectors.\n",
    "    \"\"\"\n",
    "    d = vectors_unit.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(vectors_unit)\n",
    "    return index\n",
    "\n",
    "def safe_json_dump(obj, path: str):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ---------------------------\n",
    "# Main training\n",
    "# ---------------------------\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--data\", required=True, help=\"Path to cleaned CSV (e.g., backend/data/cleaned_furniture.csv)\")\n",
    "    ap.add_argument(\"--out\", default=\"backend/storage\", help=\"Output directory for artifacts\")\n",
    "    ap.add_argument(\"--clusters\", type=int, default=12, help=\"KMeans number of clusters\")\n",
    "    ap.add_argument(\"--model\", default=\"sentence-transformers/all-MiniLM-L6-v2\", help=\"HF sentence-transformer\")\n",
    "    ap.add_argument(\"--eval_queries\", nargs=\"*\", default=[\n",
    "        \"minimalist wooden chair under 5000\",\n",
    "        \"compact study table for small room\",\n",
    "        \"king size bed modern design\",\n",
    "        \"ergonomic office chair with lumbar support\",\n",
    "        \"glass dining table for 4\"\n",
    "    ], help=\"Sample natural language queries for quick eval\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    os.makedirs(args.out, exist_ok=True)\n",
    "\n",
    "    t0 = time.time()\n",
    "    log(f\"Loading cleaned dataset: {args.data}\")\n",
    "    df = read_cleaned_csv(args.data)\n",
    "\n",
    "    # Compose corpus\n",
    "    log(\"Composing text corpus for embeddings…\")\n",
    "    corpus = [build_corpus_row(r) for _, r in df.iterrows()]\n",
    "\n",
    "    # Load embedder\n",
    "    log(f\"Loading embedder: {args.model}\")\n",
    "    emb_model = SentenceTransformer(args.model)\n",
    "\n",
    "    # Compute embeddings\n",
    "    log(\"Embedding texts… (this may take a bit)\")\n",
    "    X = embed_texts(emb_model, corpus, batch_size=128)   # [N, D]\n",
    "    d = X.shape[1]\n",
    "    log(f\"Embeddings shape: {X.shape}\")\n",
    "\n",
    "    # Normalize for cosine similarity\n",
    "    X_unit = l2_normalize(X)\n",
    "\n",
    "    # Train KMeans\n",
    "    k = min(args.clusters, len(df)) if len(df) > 1 else 1\n",
    "    if k < 2:\n",
    "        log(\"Not enough rows for clustering; skipping KMeans.\")\n",
    "        labels = np.zeros((len(df),), dtype=int)\n",
    "        sil = None\n",
    "    else:\n",
    "        log(f\"Training KMeans with k={k}…\")\n",
    "        km = KMeans(n_clusters=k, random_state=42, n_init=\"auto\")\n",
    "        labels = km.fit_predict(X)\n",
    "        # Silhouette score requires >= 2 clusters and > labels per cluster ideally\n",
    "        try:\n",
    "            sil = float(silhouette_score(X, labels))\n",
    "        except Exception:\n",
    "            sil = None\n",
    "        log(f\"KMeans trained. Silhouette: {sil}\")\n",
    "\n",
    "    # Build FAISS cosine index\n",
    "    log(\"Building FAISS cosine index…\")\n",
    "    faiss_index = build_faiss_cosine_index(X_unit)\n",
    "\n",
    "    # Save artifacts\n",
    "    emb_path = os.path.join(args.out, \"embeddings.npy\")\n",
    "    faiss_path = os.path.join(args.out, \"faiss_index.bin\")\n",
    "    meta_path = os.path.join(args.out, \"metadata.json\")\n",
    "    clusters_path = os.path.join(args.out, \"cluster_labels.csv\")\n",
    "    report_path = os.path.join(args.out, \"training_report.json\")\n",
    "\n",
    "    log(f\"Saving embeddings -> {emb_path}\")\n",
    "    np.save(emb_path, X.astype(\"float32\"))\n",
    "\n",
    "    log(f\"Saving FAISS index -> {faiss_path}\")\n",
    "    faiss.write_index(faiss_index, faiss_path)\n",
    "\n",
    "    # Build and save metadata aligned with rows\n",
    "    log(\"Saving metadata…\")\n",
    "    meta: List[Dict] = []\n",
    "    for i, r in df.reset_index(drop=True).iterrows():\n",
    "        meta.append({\n",
    "            \"row\": int(i),\n",
    "            \"uniq_id\": str(r.get(\"uniq_id\", \"\")),\n",
    "            \"title\": str(r.get(\"title_norm\", \"\")),\n",
    "            \"brand\": str(r.get(\"brand_norm\", \"\")),\n",
    "            \"price\": None if pd.isna(r.get(\"price_num\")) else float(r.get(\"price_num\")),\n",
    "            \"categories\": r.get(\"categories_list\", []),\n",
    "            \"image_url\": None if pd.isna(r.get(\"primary_image\")) else str(r.get(\"primary_image\")),\n",
    "            \"material\": str(r.get(\"material_std\", \"\")),\n",
    "            \"color\": str(r.get(\"color_std\", \"\")),\n",
    "        })\n",
    "    safe_json_dump(meta, meta_path)\n",
    "\n",
    "    # Save cluster labels\n",
    "    log(f\"Saving cluster labels -> {clusters_path}\")\n",
    "    pd.DataFrame({\"uniq_id\": df[\"uniq_id\"], \"cluster_label\": labels}).to_csv(clusters_path, index=False)\n",
    "\n",
    "    # Report\n",
    "    t1 = time.time()\n",
    "    report = {\n",
    "        \"rows\": int(len(df)),\n",
    "        \"embed_dim\": int(d),\n",
    "        \"clusters\": int(k),\n",
    "        \"silhouette\": sil,\n",
    "        \"time_sec\": round(t1 - t0, 2),\n",
    "        \"artifacts\": {\n",
    "            \"embeddings\": emb_path,\n",
    "            \"faiss_index\": faiss_path,\n",
    "            \"metadata\": meta_path,\n",
    "            \"cluster_labels\": clusters_path\n",
    "        },\n",
    "        \"model\": args.model,\n",
    "    }\n",
    "    safe_json_dump(report, report_path)\n",
    "    log(f\"Training complete in {report['time_sec']}s\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Quick eval with sample queries (cosine)\n",
    "    # ---------------------------\n",
    "    def search_top_k(query: str, top_k: int = 5):\n",
    "        q = emb_model.encode([query], convert_to_numpy=True, normalize_embeddings=False).astype(\"float32\")\n",
    "        q = l2_normalize(q)\n",
    "        scores, idxs = faiss_index.search(q, top_k)  # inner product on unit vecs = cosine\n",
    "        idxs = idxs[0].tolist()\n",
    "        scores = scores[0].tolist()\n",
    "        results = []\n",
    "        for s, j in zip(scores, idxs):\n",
    "            if j < 0:  # FAISS returns -1 if fewer than top_k\n",
    "                continue\n",
    "            results.append({\n",
    "                \"score\": float(s),\n",
    "                \"uniq_id\": meta[j][\"uniq_id\"],\n",
    "                \"title\": meta[j][\"title\"],\n",
    "                \"brand\": meta[j][\"brand\"],\n",
    "                \"price\": meta[j][\"price\"],\n",
    "                \"categories\": meta[j][\"categories\"],\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    log(\"----- Quick Eval (sample queries) -----\")\n",
    "    for q in args.eval_queries:\n",
    "        hits = search_top_k(q, top_k=5)\n",
    "        log(f\"Q: {q}\")\n",
    "        for h in hits:\n",
    "            log(f\"  • {h['title']}  | {h['brand']}  | ₹{h['price']}  | score={h['score']:.3f}\")\n",
    "    log(\"---------------------------------------\")\n",
    "\n",
    "    # Provide a tiny interactive CLI if run in a TTY\n",
    "    if sys.stdin.isatty():\n",
    "        log(\"Enter a query to search (or blank to exit):\")\n",
    "        try:\n",
    "            while True:\n",
    "                user_q = input(\"> \").strip()\n",
    "                if not user_q:\n",
    "                    break\n",
    "                for h in search_top_k(user_q, top_k=5):\n",
    "                    print(f\"  - {h['title']} | {h['brand']} | ₹{h['price']} | score={h['score']:.3f}\")\n",
    "        except (EOFError, KeyboardInterrupt):\n",
    "            pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
