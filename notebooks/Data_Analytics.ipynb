{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea3b44c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Clean furniture dataset for the Product Recommendation web app.\n",
    "\n",
    "Outputs:\n",
    "- cleaned_furniture.csv\n",
    "- cleaned_furniture.parquet (optional via --parquet)\n",
    "- analytics.json\n",
    "- schema_report.json\n",
    "\n",
    "Usage:\n",
    "  python clean_dataset.py --input intern_data_ikarus.csv --outdir backend/data --parquet\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "ASSIGNMENT_COLUMNS = [\n",
    "    \"title\",\n",
    "    \"brand\",\n",
    "    \"description\",\n",
    "    \"price\",\n",
    "    \"categories\",\n",
    "    \"images\",\n",
    "    \"manufacturer\",\n",
    "    \"package dimensions\",\n",
    "    \"country_of_origin\",\n",
    "    \"material\",\n",
    "    \"color\",\n",
    "    \"uniq_id\",\n",
    "]\n",
    "\n",
    "REQUIRED_MINIMAL = [\"uniq_id\", \"title\", \"description\", \"price\", \"categories\", \"images\", \"brand\"]\n",
    "\n",
    "\n",
    "def log(msg: str):\n",
    "    print(f\"[clean] {msg}\", flush=True)\n",
    "\n",
    "\n",
    "def coalesce_columns(df: pd.DataFrame, candidates: list[str], target: str) -> pd.Series:\n",
    "    \"\"\"Coalesce the first non-null/non-empty from candidate columns to a target Series.\"\"\"\n",
    "    s = pd.Series([None] * len(df))\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            v = df[c].astype(str).str.strip()\n",
    "            v = v.replace({\"nan\": None, \"None\": None, \"\": None})\n",
    "            s = s.fillna(v)\n",
    "    return s\n",
    "\n",
    "\n",
    "def normalize_price(value) -> float | None:\n",
    "    \"\"\"\n",
    "    Convert a price string like '₹5,999.00', '5999', '$89.90', 'Rs 2,499', '4.2k' into float (INR-like).\n",
    "    Heuristics:\n",
    "      - strip currency symbols and commas\n",
    "      - handle 'k' or 'K' as * 1000\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    s = str(value)\n",
    "    if not s or s.lower() in {\"nan\", \"none\"}:\n",
    "        return None\n",
    "\n",
    "    s = s.replace(\",\", \"\").strip()\n",
    "    # remove currency words/symbols\n",
    "    s = re.sub(r\"(rs\\.?|₹|\\$|usd|inr|eur|gbp)\", \"\", s, flags=re.IGNORECASE).strip()\n",
    "\n",
    "    # handle k/K suffix\n",
    "    m = re.fullmatch(r\"([0-9]*\\.?[0-9]+)\\s*[kK]\", s)\n",
    "    if m:\n",
    "        try:\n",
    "            return float(m.group(1)) * 1000.0\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    # extract first float number\n",
    "    m = re.search(r\"([0-9]*\\.?[0-9]+)\", s)\n",
    "    if m:\n",
    "        try:\n",
    "            return float(m.group(1))\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def pick_primary_image(images_val: str | None) -> str | None:\n",
    "    \"\"\"\n",
    "    From 'images' field that may contain:\n",
    "      - single URL\n",
    "      - JSON-like list\n",
    "      - pipe/semicolon/comma separated URLs\n",
    "    Return the first plausible URL (http/https) or None.\n",
    "    \"\"\"\n",
    "    if images_val is None or (isinstance(images_val, float) and math.isnan(images_val)):\n",
    "        return None\n",
    "    s = str(images_val).strip()\n",
    "    if not s or s.lower() in {\"nan\", \"none\"}:\n",
    "        return None\n",
    "\n",
    "    # Try JSON-like list\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        try:\n",
    "            import ast\n",
    "            arr = ast.literal_eval(s)\n",
    "            if isinstance(arr, (list, tuple)) and arr:\n",
    "                s = str(arr[0])\n",
    "            else:\n",
    "                s = \"\"\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Split by common delimiters to find URLs\n",
    "    parts = re.split(r\"[|\\;\\,\\s]+\", s)\n",
    "    for p in parts:\n",
    "        p = p.strip().strip(\"'\\\"\")\n",
    "        if p.startswith(\"http://\") or p.startswith(\"https://\"):\n",
    "            return p\n",
    "\n",
    "    # If still not found but looks like a path, return first token\n",
    "    return parts[0].strip().strip(\"'\\\"\") if parts else None\n",
    "\n",
    "\n",
    "def normalize_categories(val: str | None) -> list[str]:\n",
    "    \"\"\"\n",
    "    Normalize categories to a list of strings.\n",
    "      - split by '>', '/', '|', ',', ';'\n",
    "      - strip and title-case lightly\n",
    "    \"\"\"\n",
    "    if val is None or (isinstance(val, float) and math.isnan(val)):\n",
    "        return []\n",
    "    s = str(val)\n",
    "    if not s or s.lower() in {\"nan\", \"none\"}:\n",
    "        return []\n",
    "    tokens = re.split(r\"[>/\\|\\;\\,]+\", s)\n",
    "    tokens = [t.strip() for t in tokens if t.strip()]\n",
    "    # keep case reasonable\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def normalize_text(text: str | None, fallback: str = \"Unknown\") -> str:\n",
    "    if text is None or (isinstance(text, float) and math.isnan(text)):\n",
    "        return fallback\n",
    "    s = str(text).strip()\n",
    "    return s if s else fallback\n",
    "\n",
    "\n",
    "def standardize_color(val: str | None) -> str | None:\n",
    "    if val is None or (isinstance(val, float) and math.isnan(val)):\n",
    "        return None\n",
    "    s = str(val).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    # basic normalization\n",
    "    s = s.lower()\n",
    "    mapping = {\n",
    "        \"blk\": \"black\", \"wht\": \"white\", \"gry\": \"grey\", \"gr\": \"green\", \"brn\": \"brown\",\n",
    "        \"rd\": \"red\", \"bl\": \"blue\", \"ylw\": \"yellow\"\n",
    "    }\n",
    "    return mapping.get(s, s)\n",
    "\n",
    "\n",
    "def standardize_material(val: str | None) -> str | None:\n",
    "    if val is None or (isinstance(val, float) and math.isnan(val)):\n",
    "        return None\n",
    "    s = str(val).strip().lower()\n",
    "    if not s:\n",
    "        return None\n",
    "    replacements = {\n",
    "        \"wooden\": \"wood\",\n",
    "        \"engineered wood\": \"engineered-wood\",\n",
    "        \"mango wood\": \"wood\",\n",
    "        \"solid wood\": \"wood\",\n",
    "        \"ply\": \"plywood\",\n",
    "        \"mdf board\": \"mdf\",\n",
    "    }\n",
    "    return replacements.get(s, s)\n",
    "\n",
    "\n",
    "def ensure_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure all assignment columns exist; create empty if missing.\"\"\"\n",
    "    for col in ASSIGNMENT_COLUMNS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_schema_report(df: pd.DataFrame) -> dict:\n",
    "    rep = {\n",
    "        \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"rows\": int(len(df)),\n",
    "        \"columns\": {},\n",
    "    }\n",
    "    for c in ASSIGNMENT_COLUMNS:\n",
    "        col = df[c] if c in df.columns else pd.Series(dtype=object)\n",
    "        rep[\"columns\"][c] = {\n",
    "            \"present\": c in df.columns,\n",
    "            \"non_null\": int(col.notna().sum()),\n",
    "            \"null\": int(col.isna().sum()),\n",
    "            \"unique\": int(col.nunique(dropna=True)),\n",
    "            \"example_values\": col.dropna().astype(str).head(5).tolist(),\n",
    "            \"dtype\": str(col.dtype) if c in df.columns else None,\n",
    "        }\n",
    "    return rep\n",
    "\n",
    "\n",
    "def compute_analytics(df: pd.DataFrame) -> dict:\n",
    "    # Flatten categories for counts\n",
    "    cat_counts = Counter()\n",
    "    for cats in df[\"categories_norm\"]:\n",
    "        for c in cats:\n",
    "            cat_counts[c] += 1\n",
    "\n",
    "    # Average price by top-level category (first token)\n",
    "    topcat_prices = defaultdict(list)\n",
    "    for cats, p in zip(df[\"categories_norm\"], df[\"price_num\"]):\n",
    "        if p is None or (isinstance(p, float) and np.isnan(p)):\n",
    "            continue\n",
    "        top = cats[0] if cats else \"Uncategorized\"\n",
    "        topcat_prices[top].append(float(p))\n",
    "    avg_price_by_cat = [{\"category\": k, \"avg_price\": float(np.mean(v))} for k, v in topcat_prices.items()]\n",
    "\n",
    "    # Brand counts\n",
    "    brand_counts = Counter(df[\"brand_norm\"].fillna(\"Unknown\").tolist())\n",
    "\n",
    "    # Material distribution\n",
    "    material_counts = Counter(df[\"material_std\"].fillna(\"unknown\").tolist())\n",
    "\n",
    "    return {\n",
    "        \"categoryCounts\": [{\"category\": k, \"count\": v} for k, v in cat_counts.most_common()],\n",
    "        \"avgPriceByCategory\": sorted(avg_price_by_cat, key=lambda x: x[\"avg_price\"], reverse=True),\n",
    "        \"brandTop\": [{\"brand\": k, \"count\": v} for k, v in brand_counts.most_common(20)],\n",
    "        \"materials\": [{\"material\": k, \"count\": v} for k, v in material_counts.most_common()],\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input\", required=True, help=\"Path to raw CSV (e.g., intern_data_ikarus.csv)\")\n",
    "    parser.add_argument(\"--outdir\", default=\"backend/data\", help=\"Output directory\")\n",
    "    parser.add_argument(\"--outfile\", default=\"cleaned_furniture.csv\", help=\"Output CSV filename\")\n",
    "    parser.add_argument(\"--parquet\", action=\"store_true\", help=\"Also write Parquet\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    os.makedirs(args.outdir, exist_ok=True)\n",
    "    out_csv = os.path.join(args.outdir, args.outfile)\n",
    "    out_parquet = os.path.join(args.outdir, os.path.splitext(args.outfile)[0] + \".parquet\")\n",
    "    analytics_path = os.path.join(args.outdir, \"analytics.json\")\n",
    "    schema_path = os.path.join(args.outdir, \"schema_report.json\")\n",
    "\n",
    "    log(f\"Loading: {args.input}\")\n",
    "    try:\n",
    "        df = pd.read_csv(args.input, low_memory=False)\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(args.input, encoding=\"latin-1\", low_memory=False)\n",
    "\n",
    "    # Ensure required columns exist (create if missing)\n",
    "    df = ensure_columns(df)\n",
    "\n",
    "    # Coalesce/normalize core fields\n",
    "    # uniq_id\n",
    "    if df[\"uniq_id\"].isna().all():\n",
    "        # fallback: generate deterministic IDs from title+brand+price row index\n",
    "        df[\"uniq_id\"] = (\n",
    "            df[\"title\"].fillna(\"\").astype(str).str.slice(0, 16).str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "            + \"_\" + df.index.astype(str)\n",
    "        )\n",
    "        log(\"uniq_id was missing; generated fallback IDs.\")\n",
    "    else:\n",
    "        df[\"uniq_id\"] = df[\"uniq_id\"].astype(str).str.strip()\n",
    "\n",
    "    # title\n",
    "    df[\"title_norm\"] = df[\"title\"].apply(lambda x: normalize_text(x, \"Unknown Title\"))\n",
    "    # brand\n",
    "    df[\"brand_norm\"] = df[\"brand\"].apply(lambda x: normalize_text(x, \"Unknown\"))\n",
    "    # description\n",
    "    df[\"description_norm\"] = df[\"description\"].apply(lambda x: normalize_text(x, \"No description available.\"))\n",
    "\n",
    "    # price\n",
    "    df[\"price_num\"] = df[\"price\"].apply(normalize_price)\n",
    "\n",
    "    # categories\n",
    "    df[\"categories_norm\"] = df[\"categories\"].apply(normalize_categories)\n",
    "\n",
    "    # images -> primary_image\n",
    "    df[\"primary_image\"] = df[\"images\"].apply(pick_primary_image)\n",
    "\n",
    "    # material / color\n",
    "    df[\"material_std\"] = df[\"material\"].apply(standardize_material)\n",
    "    df[\"color_std\"] = df[\"color\"].apply(standardize_color)\n",
    "\n",
    "    # country_of_origin\n",
    "    df[\"country_of_origin_norm\"] = df[\"country_of_origin\"].apply(lambda x: normalize_text(x, \"Unknown\"))\n",
    "\n",
    "    # package dimensions -> keep as raw string; try to standardize simple patterns\n",
    "    df[\"package_dimensions_norm\"] = df[\"package dimensions\"].astype(str).str.strip()\n",
    "\n",
    "    # manufacturer\n",
    "    df[\"manufacturer_norm\"] = df[\"manufacturer\"].apply(lambda x: normalize_text(x, \"Unknown\"))\n",
    "\n",
    "    # Drop exact duplicates by uniq_id first, then by (title, brand, price)\n",
    "    before = len(df)\n",
    "    df = df.sort_values(by=[\"uniq_id\"]).drop_duplicates(subset=[\"uniq_id\"], keep=\"first\")\n",
    "    after_uid = len(df)\n",
    "    df = df.drop_duplicates(subset=[\"title_norm\", \"brand_norm\", \"price_num\"], keep=\"first\")\n",
    "    after_tbp = len(df)\n",
    "    log(f\"Dedup: {before} -> {after_uid} (uniq_id) -> {after_tbp} (title,brand,price)\")\n",
    "\n",
    "    # Sanity filter: keep rows with at least minimal information\n",
    "    df = df[~df[\"title_norm\"].isna()]\n",
    "    df = df[~df[\"description_norm\"].isna()]\n",
    "\n",
    "    # Final selected columns for the app (keeping originals where useful)\n",
    "    out_cols = [\n",
    "        \"uniq_id\",\n",
    "        \"title_norm\",\n",
    "        \"brand_norm\",\n",
    "        \"description_norm\",\n",
    "        \"price_num\",\n",
    "        \"categories_norm\",\n",
    "        \"primary_image\",\n",
    "        \"manufacturer_norm\",\n",
    "        \"package_dimensions_norm\",\n",
    "        \"country_of_origin_norm\",\n",
    "        \"material_std\",\n",
    "        \"color_std\",\n",
    "        # keep originals for reference (optional)\n",
    "        \"title\", \"brand\", \"description\", \"price\", \"categories\", \"images\", \"manufacturer\",\n",
    "        \"package dimensions\", \"country_of_origin\", \"material\", \"color\"\n",
    "    ]\n",
    "\n",
    "    # Some datasets can miss columns; filter to existing\n",
    "    out_cols = [c for c in out_cols if c in df.columns]\n",
    "\n",
    "    # Write cleaned CSV\n",
    "    log(f\"Writing cleaned CSV: {out_csv}\")\n",
    "    df[out_cols].to_csv(out_csv, index=False)\n",
    "\n",
    "    # Optional Parquet\n",
    "    if args.parquet:\n",
    "        log(f\"Writing Parquet: {out_parquet}\")\n",
    "        try:\n",
    "            df[out_cols].to_parquet(out_parquet, index=False)\n",
    "        except Exception as e:\n",
    "            log(f\"Parquet write failed (install pyarrow or fastparquet): {e}\")\n",
    "\n",
    "    # Analytics JSON (for /api/analytics)\n",
    "    log(f\"Computing analytics -> {analytics_path}\")\n",
    "    analytics = compute_analytics(df)\n",
    "    with open(analytics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(analytics, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Schema report\n",
    "    log(f\"Writing schema report -> {schema_path}\")\n",
    "    report = make_schema_report(df)\n",
    "    with open(schema_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Quick console summary\n",
    "    log(\"Summary:\")\n",
    "    log(f\"  Rows cleaned: {len(df)}\")\n",
    "    log(f\"  Example categories: {list({(c[0] if c else 'Uncategorized') for c in df['categories_norm']})[:8]}\")\n",
    "    log(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pd.options.mode.chained_assignment = None  # silence chained assignment warnings\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        log(f\"ERROR: {e}\")\n",
    "        sys.exit(1)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
